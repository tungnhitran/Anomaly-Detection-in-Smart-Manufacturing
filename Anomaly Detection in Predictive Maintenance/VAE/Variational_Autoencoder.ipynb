{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "WB7BOsBHcT7a"
   },
   "source": [
    "# Variational Autoencoder for anomaly detection in Predictive Maintenance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "id": "IWL-Zf8ZcT7v"
   },
   "outputs": [
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": false,
    "id": "FgmhxKCwcT7x",
    "outputId": "816c1c26-92b9-41a9-aaf2-cd6a65452493"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 0,
     "metadata": {
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "IW-P4jBdcT7y"
   },
   "source": [
    "### Define path to dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "id": "EwMTFxCEcT7z"
   },
   "outputs": [
   ],
   "source": [
    "load_data = pd.read_csv('train_modified.csv',sep=',')\n",
    "#del load_data['Unnamed: 0.1']\n",
    "#del load_data['Unnamed: 0']\n",
    "load_data.to_csv('train.csv')\n",
    "DATA_PATH = 'train.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "1l7lt-YUcT7z"
   },
   "source": [
    "## Dataset Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": false,
    "id": "g_MX_VN9cT70",
    "outputId": "7c81af69-448e-4674-ac99-f0473d37eaab"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'X097_DE_time', 'X097_FE_time'], dtype='object')"
      ]
     },
     "execution_count": 0,
     "metadata": {
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_base = pd.read_csv(DATA_PATH,sep=',',index_col=False)\n",
    "df_base.head()\n",
    "cols=df_base.columns\n",
    "cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "v19HBr4mcT74"
   },
   "source": [
    "## Build Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "id": "1WPr8UbmcT76"
   },
   "outputs": [
   ],
   "source": [
    "def load_and_standardize_data(path):\n",
    "    # read in from csv\n",
    "    df = pd.read_csv(path, sep=',')\n",
    "    # replace nan with 0\n",
    "    df = df.fillna(0)\n",
    "    df = df.values.reshape(-1, df.shape[1]).astype('float32')\n",
    "    # randomly split\n",
    "    X_train, X_test = train_test_split(df, test_size=0.3, random_state=42)\n",
    "    # standardize values\n",
    "    scaler = preprocessing.StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)   \n",
    "    return X_train, X_test, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "id": "Ci1G3xFIcT76"
   },
   "outputs": [
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "class DataBuilder(Dataset):\n",
    "    def __init__(self, path, train=True):\n",
    "        self.X_train, self.X_test, self.standardizer = load_and_standardize_data(DATA_PATH)\n",
    "        if train:\n",
    "            self.x = torch.from_numpy(self.X_train)\n",
    "            self.len=self.x.shape[0]\n",
    "        else:\n",
    "            self.x = torch.from_numpy(self.X_test)\n",
    "            self.len=self.x.shape[0]\n",
    "        del self.X_train\n",
    "        del self.X_test \n",
    "    def __getitem__(self,index):      \n",
    "        return self.x[index]\n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "id": "DXL-UZ5acT77"
   },
   "outputs": [
   ],
   "source": [
    "traindata_set=DataBuilder(DATA_PATH, train=True)\n",
    "testdata_set=DataBuilder(DATA_PATH, train=False)\n",
    "\n",
    "trainloader=DataLoader(dataset=traindata_set,batch_size=256)\n",
    "testloader=DataLoader(dataset=testdata_set,batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": false,
    "id": "a5VP19WEcT78",
    "outputId": "0fce79a5-a158-4098-a444-5221990b951e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Tensor, torch.Tensor)"
      ]
     },
     "execution_count": 0,
     "metadata": {
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(trainloader.dataset.x), type(testloader.dataset.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": false,
    "id": "9Gacao_rcT79",
    "outputId": "a0c2c140-d1eb-422d-ba33-ee14d6001480"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([14378, 3]), torch.Size([6162, 3]))"
      ]
     },
     "execution_count": 0,
     "metadata": {
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainloader.dataset.x.shape, testloader.dataset.x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": false,
    "id": "DqcGqKyocT79",
    "outputId": "7e031da9-82ed-4da8-e835-7029a06650f2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.7310,  0.8153, -0.9797],\n",
       "        [-1.2989,  1.5323, -1.4553],\n",
       "        [-1.3574,  0.0120, -0.6427],\n",
       "        ...,\n",
       "        [-0.8181,  1.2616, -0.5904],\n",
       "        [-1.5799,  0.1157,  0.0106],\n",
       "        [ 0.9316, -1.6551,  0.8233]])"
      ]
     },
     "execution_count": 0,
     "metadata": {
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainloader.dataset.x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "HJHBz04McT7_"
   },
   "source": [
    "## Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "id": "RKVwyk3_cT8A"
   },
   "outputs": [
   ],
   "source": [
    "class VariationalAutoencoder(nn.Module):\n",
    "    def __init__(self,D_in,H=50,H2=12,latent_dim=2):\n",
    "        \n",
    "        #Encoder\n",
    "        super(VariationalAutoencoder,self).__init__()\n",
    "        self.linear1=nn.Linear(D_in,H)\n",
    "        self.lin_bn1 = nn.BatchNorm1d(num_features=H)\n",
    "        self.linear2=nn.Linear(H,H2)\n",
    "        self.lin_bn2 = nn.BatchNorm1d(num_features=H2)\n",
    "        self.linear3=nn.Linear(H2,H2)\n",
    "        self.lin_bn3 = nn.BatchNorm1d(num_features=H2)\n",
    "        \n",
    "        # Latent vectors mu and sigma\n",
    "        self.fc1 = nn.Linear(H2, latent_dim)\n",
    "        self.bn1 = nn.BatchNorm1d(num_features=latent_dim)\n",
    "        self.fc21 = nn.Linear(latent_dim, latent_dim)\n",
    "        self.fc22 = nn.Linear(latent_dim, latent_dim)\n",
    "\n",
    "        # Sampling vector\n",
    "        self.fc3 = nn.Linear(latent_dim, latent_dim)\n",
    "        self.fc_bn3 = nn.BatchNorm1d(latent_dim)\n",
    "        self.fc4 = nn.Linear(latent_dim, H2)\n",
    "        self.fc_bn4 = nn.BatchNorm1d(H2)\n",
    "        \n",
    "        # Decoder\n",
    "        self.linear4=nn.Linear(H2,H2)\n",
    "        self.lin_bn4 = nn.BatchNorm1d(num_features=H2)\n",
    "        self.linear5=nn.Linear(H2,H)\n",
    "        self.lin_bn5 = nn.BatchNorm1d(num_features=H)\n",
    "        self.linear6=nn.Linear(H,D_in)\n",
    "        self.lin_bn6 = nn.BatchNorm1d(num_features=D_in)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def encode(self, x):\n",
    "        lin1 = self.relu(self.lin_bn1(self.linear1(x)))\n",
    "        lin2 = self.relu(self.lin_bn2(self.linear2(lin1)))\n",
    "        lin3 = self.relu(self.lin_bn3(self.linear3(lin2)))\n",
    "\n",
    "        fc1 = F.relu(self.bn1(self.fc1(lin3)))\n",
    "\n",
    "        r1 = self.fc21(fc1)\n",
    "        r2 = self.fc22(fc1)\n",
    "        \n",
    "        return r1, r2\n",
    "    \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        if self.training:\n",
    "            std = logvar.mul(0.5).exp_()\n",
    "            eps = Variable(std.data.new(std.size()).normal_())\n",
    "            return eps.mul(std).add_(mu)\n",
    "        else:\n",
    "            return mu\n",
    "        \n",
    "    def decode(self, z):\n",
    "        fc3 = self.relu(self.fc_bn3(self.fc3(z)))\n",
    "        fc4 = self.relu(self.fc_bn4(self.fc4(fc3)))\n",
    "\n",
    "        lin4 = self.relu(self.lin_bn4(self.linear4(fc4)))\n",
    "        lin5 = self.relu(self.lin_bn5(self.linear5(lin4)))\n",
    "        return self.lin_bn6(self.linear6(lin5))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "id": "_gNN4CQJcT8B"
   },
   "outputs": [
   ],
   "source": [
    "class customLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(customLoss, self).__init__()\n",
    "        self.mse_loss = nn.MSELoss(reduction=\"sum\")\n",
    "    \n",
    "    def forward(self, x_recon, x, mu, logvar):\n",
    "        loss_MSE = self.mse_loss(x_recon, x)\n",
    "        loss_KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "        return loss_MSE + loss_KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "id": "Vzv_BTadcT8B"
   },
   "outputs": [
   ],
   "source": [
    "D_in = traindata_set.x.shape[1]\n",
    "H = 50\n",
    "H2 = 12\n",
    "model = VariationalAutoencoder(D_in, H, H2).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "id": "Hyjz5GPQcT8B"
   },
   "outputs": [
   ],
   "source": [
    "loss_mse = customLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "CSIHcaUncT8C"
   },
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "id": "Ah7u2gGBcT8C"
   },
   "outputs": [
   ],
   "source": [
    "epochs = 200\n",
    "log_interval = 50\n",
    "val_losses = []\n",
    "train_losses = []\n",
    "test_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "id": "91mkdD9DcT8D"
   },
   "outputs": [
   ],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, data in enumerate(trainloader):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = model(data)\n",
    "        loss = loss_mse(recon_batch, data, mu, logvar)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "    if epoch % 50 == 0:        \n",
    "        print('====> Epoch: {} Average training loss: {:.4f}'.format(\n",
    "            epoch, train_loss / len(trainloader.dataset)))\n",
    "        train_losses.append(train_loss / len(trainloader.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "id": "MKCBf05ecT8D"
   },
   "outputs": [
   ],
   "source": [
    "def test(epoch):\n",
    "    with torch.no_grad():\n",
    "        test_loss = 0\n",
    "        for batch_idx, data in enumerate(testloader):\n",
    "            data = data.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            recon_batch, mu, logvar = model(data)\n",
    "            loss = loss_mse(recon_batch, data, mu, logvar)\n",
    "            test_loss += loss.item()\n",
    "            if epoch % 50 == 0:        \n",
    "                print('====> Epoch: {} Average test loss: {:.4f}'.format(\n",
    "                    epoch, test_loss / len(testloader.dataset)))\n",
    "            test_losses.append(test_loss / len(testloader.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": false,
    "id": "15DkSRj_cT8E",
    "outputId": "cc5adbc9-7b22-44ab-f81d-5141474289b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 50 Average training loss: 2.6004\n",
      "====> Epoch: 50 Average test loss: 0.1046\n",
      "====> Epoch: 50 Average test loss: 0.2098\n",
      "====> Epoch: 50 Average test loss: 0.3228\n",
      "====> Epoch: 50 Average test loss: 0.4323\n",
      "====> Epoch: 50 Average test loss: 0.5488\n",
      "====> Epoch: 50 Average test loss: 0.6580\n",
      "====> Epoch: 50 Average test loss: 0.7602\n",
      "====> Epoch: 50 Average test loss: 0.8641\n",
      "====> Epoch: 50 Average test loss: 0.9790\n",
      "====> Epoch: 50 Average test loss: 1.0837\n",
      "====> Epoch: 50 Average test loss: 1.1916\n",
      "====> Epoch: 50 Average test loss: 1.2969\n",
      "====> Epoch: 50 Average test loss: 1.4114\n",
      "====> Epoch: 50 Average test loss: 1.5170\n",
      "====> Epoch: 50 Average test loss: 1.6267\n",
      "====> Epoch: 50 Average test loss: 1.7329\n",
      "====> Epoch: 50 Average test loss: 1.8499\n",
      "====> Epoch: 50 Average test loss: 1.9585\n",
      "====> Epoch: 50 Average test loss: 2.0649\n",
      "====> Epoch: 50 Average test loss: 2.1773\n",
      "====> Epoch: 50 Average test loss: 2.2921\n",
      "====> Epoch: 50 Average test loss: 2.3910\n",
      "====> Epoch: 50 Average test loss: 2.5019\n",
      "====> Epoch: 50 Average test loss: 2.6028\n",
      "====> Epoch: 50 Average test loss: 2.6090\n",
      "====> Epoch: 100 Average training loss: 2.5912\n",
      "====> Epoch: 100 Average test loss: 0.1062\n",
      "====> Epoch: 100 Average test loss: 0.2128\n",
      "====> Epoch: 100 Average test loss: 0.3213\n",
      "====> Epoch: 100 Average test loss: 0.4275\n",
      "====> Epoch: 100 Average test loss: 0.5403\n",
      "====> Epoch: 100 Average test loss: 0.6514\n",
      "====> Epoch: 100 Average test loss: 0.7529\n",
      "====> Epoch: 100 Average test loss: 0.8570\n",
      "====> Epoch: 100 Average test loss: 0.9698\n",
      "====> Epoch: 100 Average test loss: 1.0708\n",
      "====> Epoch: 100 Average test loss: 1.1780\n",
      "====> Epoch: 100 Average test loss: 1.2848\n",
      "====> Epoch: 100 Average test loss: 1.3910\n",
      "====> Epoch: 100 Average test loss: 1.4947\n",
      "====> Epoch: 100 Average test loss: 1.6057\n",
      "====> Epoch: 100 Average test loss: 1.7142\n",
      "====> Epoch: 100 Average test loss: 1.8312\n",
      "====> Epoch: 100 Average test loss: 1.9403\n",
      "====> Epoch: 100 Average test loss: 2.0479\n",
      "====> Epoch: 100 Average test loss: 2.1591\n",
      "====> Epoch: 100 Average test loss: 2.2705\n",
      "====> Epoch: 100 Average test loss: 2.3694\n",
      "====> Epoch: 100 Average test loss: 2.4791\n",
      "====> Epoch: 100 Average test loss: 2.5802\n",
      "====> Epoch: 100 Average test loss: 2.5858\n",
      "====> Epoch: 150 Average training loss: 2.5878\n",
      "====> Epoch: 150 Average test loss: 0.1041\n",
      "====> Epoch: 150 Average test loss: 0.2103\n",
      "====> Epoch: 150 Average test loss: 0.3195\n",
      "====> Epoch: 150 Average test loss: 0.4300\n",
      "====> Epoch: 150 Average test loss: 0.5458\n",
      "====> Epoch: 150 Average test loss: 0.6514\n",
      "====> Epoch: 150 Average test loss: 0.7523\n",
      "====> Epoch: 150 Average test loss: 0.8513\n",
      "====> Epoch: 150 Average test loss: 0.9634\n",
      "====> Epoch: 150 Average test loss: 1.0667\n",
      "====> Epoch: 150 Average test loss: 1.1715\n",
      "====> Epoch: 150 Average test loss: 1.2791\n",
      "====> Epoch: 150 Average test loss: 1.3898\n",
      "====> Epoch: 150 Average test loss: 1.4941\n",
      "====> Epoch: 150 Average test loss: 1.6046\n",
      "====> Epoch: 150 Average test loss: 1.7088\n",
      "====> Epoch: 150 Average test loss: 1.8268\n",
      "====> Epoch: 150 Average test loss: 1.9354\n",
      "====> Epoch: 150 Average test loss: 2.0416\n",
      "====> Epoch: 150 Average test loss: 2.1524\n",
      "====> Epoch: 150 Average test loss: 2.2653\n",
      "====> Epoch: 150 Average test loss: 2.3663\n",
      "====> Epoch: 150 Average test loss: 2.4768\n",
      "====> Epoch: 150 Average test loss: 2.5775\n",
      "====> Epoch: 150 Average test loss: 2.5835\n",
      "====> Epoch: 200 Average training loss: 2.5922\n",
      "====> Epoch: 200 Average test loss: 0.1033\n",
      "====> Epoch: 200 Average test loss: 0.2109\n",
      "====> Epoch: 200 Average test loss: 0.3224\n",
      "====> Epoch: 200 Average test loss: 0.4280\n",
      "====> Epoch: 200 Average test loss: 0.5463\n",
      "====> Epoch: 200 Average test loss: 0.6570\n",
      "====> Epoch: 200 Average test loss: 0.7579\n",
      "====> Epoch: 200 Average test loss: 0.8615\n",
      "====> Epoch: 200 Average test loss: 0.9722\n",
      "====> Epoch: 200 Average test loss: 1.0750\n",
      "====> Epoch: 200 Average test loss: 1.1824\n",
      "====> Epoch: 200 Average test loss: 1.2880\n",
      "====> Epoch: 200 Average test loss: 1.3985\n",
      "====> Epoch: 200 Average test loss: 1.4994\n",
      "====> Epoch: 200 Average test loss: 1.6117\n",
      "====> Epoch: 200 Average test loss: 1.7215\n",
      "====> Epoch: 200 Average test loss: 1.8362\n",
      "====> Epoch: 200 Average test loss: 1.9498\n",
      "====> Epoch: 200 Average test loss: 2.0582\n",
      "====> Epoch: 200 Average test loss: 2.1658\n",
      "====> Epoch: 200 Average test loss: 2.2776\n",
      "====> Epoch: 200 Average test loss: 2.3771\n",
      "====> Epoch: 200 Average test loss: 2.4848\n",
      "====> Epoch: 200 Average test loss: 2.5902\n",
      "====> Epoch: 200 Average test loss: 2.5964\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, epochs + 1):\n",
    "    train(epoch)\n",
    "    test(epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "e_4BO8qDzK-x"
   },
   "source": [
    "Generate data from training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "id": "BGV_5r7acT8E"
   },
   "outputs": [
   ],
   "source": [
    "with torch.no_grad():\n",
    "    for batch_idx, data in enumerate(testloader):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = model(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "id": "Q0YDSzGQcT8F"
   },
   "outputs": [
   ],
   "source": [
    "scaler = trainloader.dataset.standardizer\n",
    "recon_row = scaler.inverse_transform(recon_batch[0].cpu().numpy())\n",
    "real_row = scaler.inverse_transform(testloader.dataset.x[0].cpu().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "id": "hXwXcx_GcT8F"
   },
   "outputs": [
   ],
   "source": [
    "df = pd.DataFrame(np.stack((recon_row, real_row)), columns = cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "id": "e3BS-ubmcT8G"
   },
   "outputs": [
   ],
   "source": [
    "sigma = torch.exp(logvar/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": false,
    "id": "moOeliODcT8G",
    "outputId": "71491be8-b29b-4086-d4ed-9fb64e57079d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-0.0014, -0.1264]), tensor([1.0033, 0.5766]))"
      ]
     },
     "execution_count": 0,
     "metadata": {
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mu[1], sigma[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": false,
    "id": "07gzDR6ZcT8H",
    "outputId": "b8eb2744-66d0-4154-bdbf-3ac4e2514dc5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-0.0001, -0.0134]), tensor([1.0008, 0.5866]))"
      ]
     },
     "execution_count": 0,
     "metadata": {
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mu.mean(axis=0), sigma.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "id": "sB0VYIRxcT8I"
   },
   "outputs": [
   ],
   "source": [
    "# sample z from q\n",
    "no_samples = traindata_set.x.shape[0]\n",
    "q = torch.distributions.Normal(mu.mean(axis=0), sigma.mean(axis=0))\n",
    "z = q.rsample(sample_shape=torch.Size([no_samples]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": false,
    "id": "dexoDmhvcT8J",
    "outputId": "bac53b11-335b-47ab-9b0b-9a90b27cbf22"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([14378, 2])"
      ]
     },
     "execution_count": 0,
     "metadata": {
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": false,
    "id": "c3KVCGXzcT8K",
    "outputId": "fc50bf7e-3e41-4e3e-ce22-7327955a225e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2228,  0.0573],\n",
       "        [-0.1123, -0.4240],\n",
       "        [ 0.2120, -0.0476],\n",
       "        [-0.4147,  0.4595],\n",
       "        [-0.2707,  0.1881]])"
      ]
     },
     "execution_count": 0,
     "metadata": {
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "id": "mV9t9ZLacT8L"
   },
   "outputs": [
   ],
   "source": [
    "with torch.no_grad():\n",
    "    pred = model.decode(z).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": false,
    "id": "Nb2oH1LVcT8L",
    "outputId": "2461679d-d48e-4e7b-eaed-47a5aaff43f4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.0462416 , -0.49644354,  0.5208957 ], dtype=float32)"
      ]
     },
     "execution_count": 0,
     "metadata": {
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "RdehQG1kcT8L"
   },
   "source": [
    "## Generate data from Variational Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": false,
    "id": "FcUAxx-CcT8M",
    "outputId": "c122b9d2-5aa3-485d-bbed-c1b1f2f888de"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14378, 3)"
      ]
     },
     "execution_count": 0,
     "metadata": {
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake_data = scaler.inverse_transform(pred)\n",
    "fake_data.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "id": "dOkUEIk_cT8M"
   },
   "outputs": [
   ],
   "source": [
    "df_fake = pd.DataFrame(fake_data)\n",
    "\n",
    "\n",
    "df_fake.columns=cols\n",
    "df_fake.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "id": "P9NFhS0bmFfd"
   },
   "outputs": [
   ],
   "source": [
    "load_datatest = pd.read_csv('test_modified.csv',sep=',')\n",
    "#del load_datatest['Unnamed: 0.1']\n",
    "#del load_datatest['Unnamed: 0']\n",
    "load_datatest.to_csv('test.csv')\n",
    "\n",
    "TEST_PATH = 'test.csv'\n",
    "df_anomaly = pd.read_csv(TEST_PATH, sep=',')\n",
    "cols_ano = df_anomaly.columns\n",
    "anomalydata_set=DataBuilder(TEST_PATH,train=True)\n",
    "anomalyloader=DataLoader(anomalydata_set,batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "id": "ndcC6qBTl-3U"
   },
   "outputs": [
   ],
   "source": [
    "#generate from anomaly data\n",
    "with torch.no_grad():\n",
    "    for batch_idx, data in enumerate(anomalyloader):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = model(data)\n",
    "\n",
    "scaler = anomalyloader.dataset.standardizer\n",
    "recon_test_row = scaler.inverse_transform(recon_batch[0].cpu().numpy())\n",
    "sigma = torch.exp(logvar/2)\n",
    "\n",
    "no_samples_test = df_anomaly.shape[0]\n",
    "q = torch.distributions.Normal(mu.mean(axis=0), sigma.mean(axis=0))\n",
    "z = q.rsample(sample_shape=torch.Size([no_samples_test]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "id": "Xy6_pUABm17D"
   },
   "outputs": [
   ],
   "source": [
    "with torch.no_grad():\n",
    "    pred_test = model.decode(z).cpu().numpy()\n",
    "fake_data_test = scaler.inverse_transform(pred_test)\n",
    "\n",
    "df_fake_test = pd.DataFrame(fake_data_test)\n",
    "\n",
    "df_fake_test.columns=cols_ano\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "id": "Ru9vYWsr9BOu"
   },
   "outputs": [
   ],
   "source": [
    "#load data from inner fault\n",
    "load_datatest = pd.read_csv('testir_modified.csv',sep=',')\n",
    "#del load_datatest['Unnamed: 0.1']\n",
    "#del load_datatest['Unnamed: 0']\n",
    "load_datatest.to_csv('testir.csv')\n",
    "\n",
    "TEST_PATH = 'testir.csv'\n",
    "df_anomaly_ir = pd.read_csv(TEST_PATH, sep=',')\n",
    "cols_ano = df_anomaly_ir.columns\n",
    "anomalydata_set=DataBuilder(TEST_PATH,train=True)\n",
    "anomalyloader=DataLoader(anomalydata_set,batch_size=256)\n",
    "\n",
    "#generate from anomaly data\n",
    "with torch.no_grad():\n",
    "    for batch_idx, data in enumerate(anomalyloader):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = model(data)\n",
    "\n",
    "scaler = anomalyloader.dataset.standardizer\n",
    "recon_test_row = scaler.inverse_transform(recon_batch[0].cpu().numpy())\n",
    "sigma = torch.exp(logvar/2)\n",
    "\n",
    "no_samples_test = df_anomaly_ir.shape[0]\n",
    "q = torch.distributions.Normal(mu.mean(axis=0), sigma.mean(axis=0))\n",
    "z = q.rsample(sample_shape=torch.Size([no_samples_test]))\n",
    "\n",
    "with torch.no_grad():\n",
    "    pred_test = model.decode(z).cpu().numpy()\n",
    "fake_data_test = scaler.inverse_transform(pred_test)\n",
    "\n",
    "df_fake_test_ir = pd.DataFrame(fake_data_test)\n",
    "\n",
    "df_fake_test_ir.columns=cols_ano\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "id": "YYfR2I0X954E"
   },
   "outputs": [
   ],
   "source": [
    "#load data from outer fault\n",
    "load_datatest = pd.read_csv('testor_modified.csv',sep=',')\n",
    "#del load_datatest['Unnamed: 0.1']\n",
    "#del load_datatest['Unnamed: 0']\n",
    "load_datatest.to_csv('testor.csv')\n",
    "\n",
    "TEST_PATH = 'testor.csv'\n",
    "df_anomaly_or = pd.read_csv(TEST_PATH, sep=',')\n",
    "cols_ano = df_anomaly_or.columns\n",
    "anomalydata_set=DataBuilder(TEST_PATH,train=True)\n",
    "anomalyloader=DataLoader(anomalydata_set,batch_size=256)\n",
    "\n",
    "#generate from anomaly data\n",
    "with torch.no_grad():\n",
    "    for batch_idx, data in enumerate(anomalyloader):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = model(data)\n",
    "\n",
    "scaler = anomalyloader.dataset.standardizer\n",
    "recon_test_row = scaler.inverse_transform(recon_batch[0].cpu().numpy())\n",
    "sigma = torch.exp(logvar/2)\n",
    "\n",
    "no_samples_test = df_anomaly_or.shape[0]\n",
    "q = torch.distributions.Normal(mu.mean(axis=0), sigma.mean(axis=0))\n",
    "z = q.rsample(sample_shape=torch.Size([no_samples_test]))\n",
    "\n",
    "with torch.no_grad():\n",
    "    pred_test = model.decode(z).cpu().numpy()\n",
    "fake_data_test = scaler.inverse_transform(pred_test)\n",
    "\n",
    "df_fake_test_or = pd.DataFrame(fake_data_test)\n",
    "\n",
    "df_fake_test_or.columns=cols_ano\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "id": "JnrpvYFFiR-h"
   },
   "outputs": [
   ],
   "source": [
    "#reconstruction error train data\n",
    "mse = ((df_base-df_fake)**2).mean(axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "id": "Ofv7FjxZh6Cg"
   },
   "outputs": [
   ],
   "source": [
    "#reconstruction error ball data\n",
    "mse_test = ((df_anomaly-df_fake_test)**2).mean(axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "id": "GdDqWthR-H9T"
   },
   "outputs": [
   ],
   "source": [
    "#reconstruction error inner data\n",
    "mse_test_ir = ((df_anomaly_ir-df_fake_test_ir)**2).mean(axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "id": "q73pvgq5_IZU"
   },
   "outputs": [
   ],
   "source": [
    "#reconstruction error outer data\n",
    "mse_test_or = ((df_anomaly_or-df_fake_test_or)**2).mean(axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "id": "35bub1obxrvm"
   },
   "outputs": [
   ],
   "source": [
    "#calculate anomaly threshold from reconstruction error train data\n",
    "threshold = mse.max()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "\n",
    "figure(figsize=(10, 6), dpi=80)\n",
    "plt.plot(mse[0:200],'go',label='normal')\n",
    "plt.plot(mse_test[0:200],'bD',label='ball')\n",
    "plt.plot(mse_test_ir[0:200],'r^',label='inner')\n",
    "plt.plot(mse_test_or[0:200],'k*',label='outer')\n",
    "plt.axhline(y=threshold,color='r', linestyle='-',label='threshold')\n",
    "plt.legend(loc=4)\n",
    "plt.xlabel(\"Observations\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.title('Anomaly Detection Results')\n",
    "plt.savefig('AD.pdf')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Copy of tabular-data-variational-autoencoder.ipynb",
   "provenance": [
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (system-wide)",
   "language": "python",
   "metadata": {
    "cocalc": {
     "description": "Python 3 programming language",
     "priority": 100,
     "url": "https://www.python.org/"
    }
   },
   "name": "python3",
   "resource_dir": "/ext/jupyter/kernels/python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}