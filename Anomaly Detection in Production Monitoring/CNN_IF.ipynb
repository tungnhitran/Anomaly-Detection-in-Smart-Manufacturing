{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN-IF.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "l7YDHMB1D294"
      },
      "source": [
        "# for loading/processing the images  \n",
        "from keras.preprocessing.image import load_img \n",
        "from keras.preprocessing.image import img_to_array \n",
        "from keras.applications.vgg16 import preprocess_input \n",
        "\n",
        "# models \n",
        "from keras.applications.vgg16 import VGG16 \n",
        "from keras.models import Model\n",
        "\n",
        "# clustering and dimension reduction\n",
        "# from sklearn.cluster import KMeans\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# for everything else\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from random import randint\n",
        "import pandas as pd\n",
        "import pickle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_EfvqcKlKc1y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a3489a8-b2a0-46bf-da69-65cadabb8aae"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_1Y4lbmdL1bh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d293e03-2c4b-418f-92fb-72c1e794699c"
      },
      "source": [
        "\n",
        "# load the model first and pass as an argument, remove the output layer\n",
        "model = VGG16()\n",
        "model = Model(inputs = model.inputs, outputs = model.layers[-2].output)\n",
        "\n",
        "def extract_features(file, model):\n",
        "    # load the image as a 224x224 array\n",
        "    img = load_img(file, target_size=(224,224), interpolation='bicubic')\n",
        "    # convert from 'PIL.Image.Image' to numpy array\n",
        "    img = np.array(img) \n",
        "    # reshape the data for the model reshape(num_of_samples, dim 1, dim 2, channels)\n",
        "    reshaped_img = img.reshape(1,224,224,3) \n",
        "    # prepare image for model\n",
        "    imgx = preprocess_input(reshaped_img)\n",
        "    # get the feature vector\n",
        "    features = model.predict(imgx, use_multiprocessing=True)\n",
        "    return features"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels.h5\n",
            "553467904/553467096 [==============================] - 6s 0us/step\n",
            "553476096/553467096 [==============================] - 6s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MYhtzV_VKnbe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "516ceaf8-2f10-42d6-a515-2fccc9f0d681"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Check the root folder\n",
        "!ls /content/drive/\n",
        "\n",
        "# Path should be visually same as in drive.google.com \n",
        "!ls /content/drive/MyDrive/CS1/trainingimage\n",
        "\n",
        "path = \"\" #path to training image\n",
        "# change the working directory to the path where the images are located\n",
        "os.chdir(path)\n",
        "# this list holds all the image filename\n",
        "nutImg = []\n",
        "# creates a ScandirIterator aliased as files\n",
        "with os.scandir(path) as files:\n",
        "  # loops through each file in the directory\n",
        "    for file in files:\n",
        "        if file.name.endswith('.png'):\n",
        "            # adds only the image files to the fashion list\n",
        "            nutImg.append(file.name)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "MyDrive\n",
            "100.png  19.png  28.png  37.png  46.png  55.png  64.png  73.png  82.png  91.png\n",
            "10.png\t 1.png\t 29.png  38.png  47.png  56.png  65.png  74.png  83.png  92.png\n",
            "11.png\t 20.png  2.png\t 39.png  48.png  57.png  66.png  75.png  84.png  93.png\n",
            "12.png\t 21.png  30.png  3.png\t 49.png  58.png  67.png  76.png  85.png  94.png\n",
            "13.png\t 22.png  31.png  40.png  4.png\t 59.png  68.png  77.png  86.png  95.png\n",
            "14.png\t 23.png  32.png  41.png  50.png  5.png\t 69.png  78.png  87.png  96.png\n",
            "15.png\t 24.png  33.png  42.png  51.png  60.png  6.png\t 79.png  88.png  97.png\n",
            "16.png\t 25.png  34.png  43.png  52.png  61.png  70.png  7.png\t 89.png  98.png\n",
            "17.png\t 26.png  35.png  44.png  53.png  62.png  71.png  80.png  8.png\t 99.png\n",
            "18.png\t 27.png  36.png  45.png  54.png  63.png  72.png  81.png  90.png  9.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zzkRcbtiL9xa"
      },
      "source": [
        "\n",
        "data = {}\n",
        "\n",
        "# lop through each image in the dataset\n",
        "for nut in nutImg:\n",
        "    # try to extract the features and update the dictionary\n",
        "    try:\n",
        "        feat = extract_features(nut,model)\n",
        "        data[nut] = feat\n",
        "    # if something fails, save the extracted features as a pickle file (optional)\n",
        "    except:\n",
        "        with open(p,'wb') as file:\n",
        "            pickle.dump(data,file)\n",
        "            \n",
        "# get a list of the filenames\n",
        "filenames = np.array(list(data.keys()))\n",
        "\n",
        "# get a list of just the features\n",
        "feat_train = np.array(list(data.values()))\n",
        "feat_train.shape\n",
        "\n",
        "# reshape so that there are n images of 4096 vectors\n",
        "feat_train = feat_train.reshape(-1,4096)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IKgYjxQUNf8t"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "path = \"\" #path to normal testing image\n",
        "# change the working directory to the path where the images are located\n",
        "os.chdir(path)\n",
        "# this list holds all the image filename\n",
        "nutImg_test_normal = []\n",
        "# creates a ScandirIterator aliased as files\n",
        "with os.scandir(path) as files:\n",
        "  # loops through each file in the directory\n",
        "    for file in files:\n",
        "        if file.name.endswith('.png'):\n",
        "            # adds only the image files to the fashion list\n",
        "            nutImg_test_normal.append(file.name)\n",
        "data = {}\n",
        "\n",
        "# lop through each image in the dataset\n",
        "for nut in nutImg_test_normal:\n",
        "    # try to extract the features and update the dictionary\n",
        "    try:\n",
        "        feat = extract_features(nut,model)\n",
        "        data[nut] = feat\n",
        "    # if something fails, save the extracted features as a pickle file (optional)\n",
        "    except:\n",
        "        with open(p,'wb') as file:\n",
        "            pickle.dump(data,file)\n",
        "            \n",
        "# get a list of the filenames\n",
        "filenames = np.array(list(data.keys()))\n",
        "\n",
        "# get a list of just the features\n",
        "feat_test_normal = np.array(list(data.values()))\n",
        "feat_test_normal.shape\n",
        "\n",
        "# reshape so that there are n images of 4096 vectors\n",
        "feat_test_normal = feat_test_normal.reshape(-1,4096)\n",
        "feat_test_normal.shape\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4-xeCH8IX64o"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "\n",
        "path = \"\" ##path to anomaly testing image\n",
        "# change the working directory to the path where the images are located\n",
        "os.chdir(path)\n",
        "# this list holds all the image filename\n",
        "nutImg_test_anomal = []\n",
        "# creates a ScandirIterator aliased as files\n",
        "with os.scandir(path) as files:\n",
        "  # loops through each file in the directory\n",
        "    for file in files:\n",
        "        if file.name.endswith('.png'):\n",
        "            # adds only the image files to the fashion list\n",
        "            nutImg_test_anomal.append(file.name)\n",
        "data = {}\n",
        "\n",
        "# lop through each image in the dataset\n",
        "for nut in nutImg_test_anomal:\n",
        "    # try to extract the features and update the dictionary\n",
        "    try:\n",
        "        feat = extract_features(nut,model)\n",
        "        data[nut] = feat\n",
        "    # if something fails, save the extracted features as a pickle file (optional)\n",
        "    except:\n",
        "        with open(p,'wb') as file:\n",
        "            pickle.dump(data,file)\n",
        "            \n",
        "# get a list of the filenames\n",
        "filenames = np.array(list(data.keys()))\n",
        "\n",
        "# get a list of just the features\n",
        "feat_test_anomal = np.array(list(data.values()))\n",
        "feat_test_anomal.shape\n",
        "\n",
        "# reshape so that there are n images of 4096 vectors\n",
        "feat_test_anomal = feat_test_anomal.reshape(-1,4096)\n",
        "feat_test_anomal.shape\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hv_bdFqWSDA-"
      },
      "source": [
        "#Isolation Forest\n",
        "from sklearn.ensemble import IsolationForest\n",
        "\n",
        "X_train = feat_train\n",
        "model = IsolationForest(contamination=.04,random_state=0).fit(X_train)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cCRbvU1nSXWT",
        "outputId": "49983236-880c-4714-d813-47be326b5f49"
      },
      "source": [
        "X_test = np.concatenate((feat_test_normal, feat_test_anomal), axis = 0)\n",
        "\n",
        "from sklearn import metrics  \n",
        "\n",
        "y_test_normal = np.ones(feat_test_normal.shape[0]).reshape(feat_test_normal.shape[0], 1)\n",
        "y_test_anomal = -1 * np.ones(feat_test_anomal.shape[0]).reshape(feat_test_anomal.shape[0], 1)\n",
        "\n",
        "y_test = np.concatenate((y_test_normal, y_test_anomal), axis=0)\n",
        "\n",
        "preds_forest = model.predict(X_test)  \n",
        "targs_forest = y_test\n",
        "\n",
        "preds_forest = preds_forest*-1\n",
        "targs_forest = targs_forest*-1\n",
        "\n",
        "print(\"accuracy: \", metrics.accuracy_score(targs_forest, preds_forest))  \n",
        "print(\"precision: \", metrics.precision_score(targs_forest, preds_forest))  \n",
        "print(\"recall: \", metrics.recall_score(targs_forest, preds_forest))  \n",
        "print(\"f1: \", metrics.f1_score(targs_forest, preds_forest))  \n",
        "print(\"area under curve (auc): \", metrics.roc_auc_score(targs_forest, preds_forest))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy:  0.9583333333333334\n",
            "precision:  0.92\n",
            "recall:  0.9583333333333334\n",
            "f1:  0.9387755102040817\n",
            "area under curve (auc):  0.9583333333333335\n"
          ]
        }
      ]
    }
  ]
}